{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hycCjY3iJIPs",
        "outputId": "f02d97a3-e283-4036-c1ea-7142c54a3002"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAnk76ECJFYB",
        "outputId": "c0092f54-68ba-49f4-b65c-818fcd308846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n",
            "Using GPU:  \n",
            "Warning: Folder Non-Glaucomous not found at my_glaucoma_dataset\\Non-Glaucomous\n",
            "Warning: Folder Glaucomous not found at my_glaucoma_dataset\\Glaucomous\n",
            "Total images loaded: 0\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m image_paths, labels \u001b[38;5;241m=\u001b[39m load_dataset(dataset_path, max_images_per_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Split dataset\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m X_train_paths, X_val_paths, X_test_paths, y_train, y_val, y_test \u001b[38;5;241m=\u001b[39m \u001b[43msplit_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[0;32m     87\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m prepare_data(X_train_paths, y_train)\n",
            "Cell \u001b[1;32mIn[6], line 53\u001b[0m, in \u001b[0;36msplit_dataset\u001b[1;34m(image_paths, labels, test_size, val_size)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_dataset\u001b[39m(image_paths, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, val_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[1;32m---> 53\u001b[0m     X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     X_val, X_test, y_val, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_temp, y_temp, test_size\u001b[38;5;241m=\u001b[39mval_size, stratify\u001b[38;5;241m=\u001b[39my_temp, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_val, X_test, y_train, y_val, y_test\n",
            "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
            "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "\n",
        "# Enable GPU logging for device placement\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(\"Using GPU: \", tf.test.gpu_device_name())\n",
        "\n",
        "def load_dataset(dataset_path, max_images_per_class=2000):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    class_mapping = {'Non-Glaucomous': 0, 'Glaucomous': 1}  # Update based on folder names\n",
        "\n",
        "    for class_name, label in class_mapping.items():\n",
        "        folder_path = os.path.join(dataset_path, class_name)\n",
        "        if not os.path.exists(folder_path):\n",
        "            print(f\"Warning: Folder {class_name} not found at {folder_path}\")\n",
        "            continue\n",
        "\n",
        "        all_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        all_files = sorted(all_files)[:max_images_per_class]\n",
        "\n",
        "        for filename in all_files:\n",
        "            image_path = os.path.join(folder_path, filename)\n",
        "            image_paths.append(image_path)\n",
        "            labels.append(label)\n",
        "\n",
        "    print(f\"Total images loaded: {len(image_paths)}\")\n",
        "    return image_paths, labels\n",
        "\n",
        "def preprocess_image(image_path, target_size=(224, 224)):\n",
        "    image = cv2.imread(image_path)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    resized_image = cv2.resize(image_rgb, target_size, interpolation=cv2.INTER_AREA)\n",
        "    normalized_image = resized_image.astype(np.float32) / 255.0\n",
        "    return normalized_image\n",
        "\n",
        "def prepare_data(image_paths, labels, target_size=(224, 224)):\n",
        "    images = [preprocess_image(path, target_size) for path in image_paths]\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    return images, labels\n",
        "\n",
        "def split_dataset(image_paths, labels, test_size=0.2, val_size=0.5):\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(image_paths, labels, test_size=test_size, stratify=labels, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_size, stratify=y_temp, random_state=42)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "def create_vgg16_model(input_shape=(224, 224, 3)):\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, outputs)\n",
        "    model.compile(optimizer=Adam(1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Mount Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Dataset path\n",
        "dataset_path = \"my_glaucoma_dataset\"\n",
        "\n",
        "# Define the path to save the model\n",
        "model_save_path = \"Models/my_glaucoma_dataset/vgg16_glaucoma_model.h5\"\n",
        "\n",
        "# Load dataset\n",
        "image_paths, labels = load_dataset(dataset_path, max_images_per_class=2000)\n",
        "\n",
        "# Split dataset\n",
        "X_train_paths, X_val_paths, X_test_paths, y_train, y_val, y_test = split_dataset(image_paths, labels)\n",
        "\n",
        "# Preprocess data\n",
        "X_train, y_train = prepare_data(X_train_paths, y_train)\n",
        "X_val, y_val = prepare_data(X_val_paths, y_val)\n",
        "X_test, y_test = prepare_data(X_test_paths, y_test)\n",
        "\n",
        "# Create and train VGG16 model\n",
        "input_shape = (224, 224, 3)\n",
        "vgg16_model = create_vgg16_model(input_shape)\n",
        "\n",
        "print(\"\\nTraining VGG16 model...\")\n",
        "history_vgg16 = vgg16_model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=16,\n",
        "    batch_size=8,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the model in TensorFlow format\n",
        "print(\"\\nSaving the TensorFlow model to Google Drive...\")\n",
        "vgg16_model.save(model_save_path)\n",
        "print(f\"TensorFlow model saved at {model_save_path}\")\n",
        "\n",
        "# Save the model as a .pkl file\n",
        "pkl_model_save_path = \"Model/vgg16_glaucoma_model.pkl\"\n",
        "print(\"\\nSaving the model as a .pkl file...\")\n",
        "with open(pkl_model_save_path, 'wb') as f:\n",
        "    pickle.dump(vgg16_model, f)\n",
        "print(f\"Pickle model saved at {pkl_model_save_path}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = vgg16_model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"\\nTest Accuracy: {test_accuracy:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEHxydEdnTmY",
        "outputId": "22452689-bef6-43dc-e6f0-9a9199e922fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001531D41BC70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001531D41BC70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 429ms/step\n",
            "Prediction: Non-Glaucomous\n",
            "Confidence: 99.83%\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Path to the saved model\n",
        "model_save_path = \"Model/vgg16_glaucoma_model.h5\"\n",
        "\n",
        "# Load the saved model\n",
        "vgg16_model = load_model(model_save_path)\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Function to preprocess a single image\n",
        "def preprocess_input_image(image_path, target_size=(224, 224)):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Image not found at path: {image_path}\")\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    resized_image = cv2.resize(image_rgb, target_size, interpolation=cv2.INTER_AREA)\n",
        "    normalized_image = resized_image.astype(np.float32) / 255.0\n",
        "    return np.expand_dims(normalized_image, axis=0)  # Add batch dimension\n",
        "\n",
        "# Function to make a prediction\n",
        "def predict_image_class(image_path):\n",
        "    # Preprocess the image\n",
        "    preprocessed_image = preprocess_input_image(image_path)\n",
        "\n",
        "    # Make prediction\n",
        "    prediction = vgg16_model.predict(preprocessed_image)\n",
        "\n",
        "    # Interpret the result\n",
        "    class_mapping = {0: \"Non-Glaucomous\", 1: \"Glaucomous\"}\n",
        "    predicted_class = int(np.round(prediction[0][0]))  # Convert probability to binary class\n",
        "    confidence = prediction[0][0] * 100 if predicted_class == 1 else (1 - prediction[0][0]) * 100\n",
        "\n",
        "    print(f\"Prediction: {class_mapping[predicted_class]}\")\n",
        "    print(f\"Confidence: {confidence:.2f}%\")\n",
        "    return class_mapping[predicted_class], confidence\n",
        "\n",
        "# Example usage\n",
        "image_path = \"my_glaucoma_dataset/my_glaucoma_dataset/Non-Glaucomous/002_vflip.jpg\"  # Replace with the path to your input image\n",
        "predicted_class, confidence = predict_image_class(image_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
